# Stage 2: Preference Data Generation

Generate high-quality preference pairs with clear preference signal for DPO training.

---

## Overview

**Purpose**: Create preference pairs (chosen vs rejected responses) that teach the model desired behaviors.

**Goal**: Collect ≥1000 pairs with preference gap ≥0.15 and decontamination ≥0.9.

**Key Principle**: Preference data quality directly determines DPO effectiveness. Invest heavily in this stage.

---

## Inputs

- **Prompts/instructions**: Diverse set of user queries (≥1000 unique)
- **Base model**: SFT model from Stage 1 for response generation
- **Annotation resources**: Human annotators or reward model

---

## Process

### 1. Collect Prompts

**Sources**:
- User queries from production logs (anonymized)
- Synthetic prompts generated by strong LLM
- Curated datasets (Anthropic HH, OpenAssistant)
- Domain-specific examples

**Diversity requirements**:
- Multiple categories (QA, reasoning, creative, coding, etc.)
- Varying difficulty levels
- Different formats (short/long, structured/unstructured)
- Edge cases and challenging scenarios

**Quality checks**:
- Clear and unambiguous prompts
- Appropriate difficulty (not too easy/hard)
- Safe content (no harmful requests)
- Representative of target use cases

### 2. Generate Response Candidates

**Strategy 1: Sample from base model**
```python
# Generate multiple candidates per prompt
candidates = model.generate(
    prompt,
    num_return_sequences=4,
    temperature=0.8,  # Higher for diversity
    top_p=0.9,
    max_length=512
)
```

**Strategy 2: Multiple models**
- Use different models or checkpoints
- Vary sampling parameters
- Apply different prompting strategies

**Strategy 3: Hybrid**
- Mix model generations with human-written responses
- Use strong model for "chosen", weaker for "rejected"

**Goal**: Create response pairs with clear quality difference.

### 3. Annotate Preferences

**Human annotation**:
- Show annotators: prompt + 2 responses
- Ask: "Which response is better?"
- Criteria: helpfulness, harmlessness, honesty
- Guidelines: Clear annotation instructions

**AI annotation** (using strong reward model):
- Score responses with reward model
- Select high-score as "chosen", low-score as "rejected"
- Validate sample with human review

**Quality control**:
- Multiple annotators per pair
- Inter-annotator agreement measurement
- Regular calibration sessions
- Spot-check difficult examples

### 4. Handle Conflicting Preferences

**Types of conflicts**:

**Inconsistent annotations**:
- Same pair, different annotators, different preferences
- **Solution**: Majority vote or use tie-breaker annotator

**Contradictory examples**:
- Similar prompts with opposite preference directions
- **Solution**: Review and remove or resolve inconsistency

**Domain disagreements**:
- Different valid preferences based on context
- **Solution**: Add context to prompts or create separate datasets

**Low-confidence pairs**:
- Annotators unsure which is better
- **Solution**: Discard pairs with low agreement (<60%)

### 5. Validate Preference Pairs

**Use training_metrics.py library**:
```python
from pathlib import Path
from training_metrics import validate_dpo_pairs

# Validate preference pairs
metrics = validate_dpo_pairs(
    dpo_path=Path("preference_pairs.jsonl"),
    gap_threshold=0.15
)

print(f"Preference Gap: {metrics.avg_gap:.4f} (target ≥0.15)")
print(f"Total Pairs: {metrics.total_pairs} (target ≥1000)")
print(f"Decontamination: {metrics.decontamination:.4f} (target ≥0.9)")
print(f"Quality Score: {metrics.quality_score:.2f}")

if metrics.avg_gap < 0.15:
    print("⚠️ Preference gap too low - generate more diverse examples")
if metrics.total_pairs < 1000:
    print("⚠️ Insufficient pairs - collect more data")
if metrics.decontamination < 0.9:
    print("⚠️ Eval contamination detected - remove contaminated examples")
```

**Integration with preference-data-quality skill**:
- Use **preference-data-quality** skill for DPO metrics
- Reference thresholds: gap ≥0.15, KL ≤0.1, decontamination ≥0.9
- Cross-validate quality scores

### 6. Create Data Splits

**Recommended split**:
- **Training**: 80% (≥800 pairs)
- **Validation**: 20% (≥200 pairs)

**Stratification**:
- Ensure prompt diversity in both splits
- Balance categories across splits
- Keep similar prompts in same split (prevent leakage)

### 7. Format and Save Data

**JSONL format**:
```jsonl
{"prompt": "Explain quantum computing to a beginner.", "chosen": "Quantum computing uses quantum mechanics principles like superposition and entanglement to process information. Think of it like this: a regular computer bit is like a coin that's either heads or tails. A quantum bit (qubit) is like a spinning coin that's both at once until you look at it. This lets quantum computers explore many solutions simultaneously.", "rejected": "Quantum computing is a type of computing that uses quantum mechanics."}
{"prompt": "Write Python code to reverse a string.", "chosen": "Here's how to reverse a string in Python:\n\n```python\ndef reverse_string(s):\n    return s[::-1]\n\n# Example\noriginal = 'hello'\nreversed_str = reverse_string(original)\nprint(reversed_str)  # Output: 'olleh'\n```\n\nThe `[::-1]` slice notation creates a reversed copy of the string.", "rejected": "```python\ns = 'hello'\nprint(s[::-1])\n```"}
```

**Quality checks before saving**:
- Valid JSON on each line
- All required fields present (prompt, chosen, rejected)
- No empty strings
- Character encoding correct (UTF-8)

---

## Outputs

- ✅ **Validated preference pairs**: JSONL format with quality metrics
- ✅ **Data splits**: Training and validation sets
- ✅ **Quality metrics**: Gap, count, decontamination scores
- ✅ **Data statistics report**: Diversity, categories, annotations

---

## Quality Gate

**Pass criteria**:
- Preference gap ≥0.15
- Minimum pairs ≥1000
- Decontamination ≥0.9
- Data splits created
- Format validated

**Failure handling**:
- If gap <0.15 → Generate more diverse responses, improve annotation
- If pairs <1000 → Collect more prompts and annotations
- If decontamination <0.9 → Remove contaminated examples
- If format invalid → Fix and re-validate

---

## Time Estimate

- **Human annotation**: 3-7 days for 1000 pairs (depending on annotator availability)
- **AI annotation**: 1-2 days for 1000 pairs (faster but may need human validation)
- **Validation and formatting**: 1 day

**Total**: 3-7 days for complete preference dataset

---

## Common Issues

### Issue 1: Low Preference Gap (<0.15)

**Symptoms**: Model cannot distinguish between chosen and rejected

**Causes**:
- Responses too similar
- Annotation criteria unclear
- Insufficient response diversity

**Solutions**:
- Generate more diverse candidates (increase temperature)
- Use different models for chosen/rejected
- Clarify annotation guidelines
- Add more extreme examples (very good vs very bad)

### Issue 2: Insufficient Data (<1000 pairs)

**Symptoms**: Not enough training examples

**Causes**:
- Limited annotation budget
- Slow annotation process
- High rejection rate due to quality filters

**Solutions**:
- Use AI annotation for initial pass
- Parallelize annotation across multiple annotators
- Relax quality filters slightly (maintain gap ≥0.15)
- Use data augmentation techniques

### Issue 3: Inconsistent Preferences

**Symptoms**: Contradictory annotations, low agreement

**Causes**:
- Unclear annotation guidelines
- Subjective or ambiguous examples
- Annotator fatigue

**Solutions**:
- Improve annotation guidelines with examples
- Regular calibration sessions
- Remove ambiguous examples
- Use multiple annotators per example

### Issue 4: Eval Contamination (decontamination <0.9)

**Symptoms**: Preference prompts overlap with evaluation benchmarks

**Causes**:
- Prompts sourced from public datasets
- Accidental inclusion of benchmark examples

**Solutions**:
- Decontaminate against benchmark datasets
- Remove exact matches and near-duplicates
- Use original prompts when possible

---

## Best Practices

1. **Diversity over volume** - 1000 diverse pairs > 10K similar pairs
2. **Clear preference signal** - Maximize quality difference between chosen/rejected
3. **Annotation quality** - Invest in clear guidelines and training
4. **Validate continuously** - Check metrics throughout collection
5. **Handle edge cases** - Include challenging and ambiguous examples
6. **Document decisions** - Record annotation criteria and edge case resolutions
7. **Iterate** - Start small, validate quality, then scale up

---

## Preference Pair Quality Matrix

| Quality Level | Gap | Agreement | Description |
|---------------|-----|-----------|-------------|
| **Excellent** | ≥0.30 | ≥90% | Clear, unambiguous preferences |
| **Good** | 0.15-0.30 | 70-90% | Acceptable for DPO |
| **Marginal** | 0.10-0.15 | 50-70% | Use with caution |
| **Poor** | <0.10 | <50% | Discard or re-annotate |

---

## Data Augmentation Techniques

When insufficient pairs available:

**Technique 1: Response editing**
- Take rejected response, manually improve to create chosen
- Ensures clear quality difference

**Technique 2: Synthetic generation**
- Use strong LLM to generate chosen responses
- Use weaker model or corrupted prompt for rejected

**Technique 3: Contrast pairs**
- Generate response following guidelines (chosen)
- Generate response violating guidelines (rejected)

**Warning**: Augmentation can introduce artifacts. Always validate with real annotations.

---

## Related Documentation

- `../workflow.md` - Complete workflow overview
- `quality-thresholds.md` - Detailed threshold definitions
- `model-initialization.md` - Next stage: Model setup
- `../templates.md` - Data format templates
- `../SKILL.md` - Integration with preference-data-quality skill
