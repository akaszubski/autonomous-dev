# Testing Strategy: The Diamond Model

**Last Updated**: 2026-02-17

## Overview

autonomous-dev uses a **diamond testing model** — not the traditional testing pyramid. The diamond emphasizes both extremes: deterministic hard floors (bottom) and semantic acceptance criteria (top), with generated/probabilistic layers in the middle.

```
     /  Acceptance Criteria  \     Human-defined, LLM-as-judge evaluated
    / LLM-as-Judge Eval Layer \    Probabilistic, ~80-90% human agreement
   / Integration & Contract    \   Generated from acceptance criteria
   \ Property-Based Invariants /   "Output must be valid JSON", etc.
    \ Deterministic Unit Tests/    Generated by agent, constrained by above
     \  Type System / Lints  /     Hard floor, zero tolerance
```

**Why not a pyramid?** The traditional pyramid (many unit tests, few E2E) was designed for human-speed code generation where unit tests were cheapest. With agent-speed generation, acceptance criteria are the cheapest form of meaningful specification. Unit tests generated by agents are useful as regression locks but poor as specifications — agents game them.

---

## The Six Layers

### Layer 1: Type System & Lints (Hard Floor)

**What**: Static analysis, formatting, structure validation
**Speed**: < 1 second
**Determinism**: 100%
**Runs**: Every commit (CI gate)

**autonomous-dev components**:
- Pre-commit hooks: `auto_format.py` (black, isort)
- Structure enforcement: `validate_command_file_ops.py`
- Security scanning: `security_scan.py`

**Principle**: Zero tolerance. If code doesn't typecheck or lint, it doesn't proceed. This constrains agents, not the other way around.

### Layer 2: Deterministic Unit Tests

**What**: Individual functions in isolation
**Speed**: < 5 seconds (smoke), < 30 seconds (full)
**Determinism**: 100%
**Runs**: Every commit (CI gate)

**autonomous-dev components**:
- `tests/unit/` — pure function tests, mocked dependencies
- `tests/regression/smoke/` — critical path, < 5s
- `tests/regression/progression/` — issue-specific regression prevention

**Role in diamond**: Regression locks, not specifications. These ensure implemented behavior doesn't change, but they don't define what "correct" means. Agent-generated unit tests lock in the implementation after acceptance criteria are satisfied.

**HARD GATE**: `test_dynamic_component_counts.py` is the gold standard — uses `glob()` discovery, minimum thresholds, structural checks. No hardcoded counts (`assert len(agents) == 16` is FORBIDDEN).

### Layer 3: Property-Based Invariants

**What**: Universal properties that hold across all inputs
**Speed**: < 10 seconds
**Determinism**: 100% (properties are deterministic, input generation is exhaustive)
**Runs**: Every commit (CI gate)

**Examples**:
- "Output must never contain PII"
- "JSON serialization round-trips to identity"
- "Tool calls must follow preconditions"
- "Hook must always exit (never hang)"

**autonomous-dev components**:
- Hook exit code enforcement (hooks always exit 0 or 1, never hang)
- Structure invariants (every hook in settings must exist on disk)
- Manifest sync (install_manifest.json matches source files)

**Research data**: Property-Generated Solver (PGS) framework shows +23-37% improvement in pass@1 over traditional TDD (arXiv 2506.18315). LLMs excel at discovering properties from function names, docstrings, and call patterns.

### Layer 4: Integration & Contract Tests

**What**: Components working together, API contracts, workflow validation
**Speed**: < 30 seconds
**Determinism**: 100%
**Runs**: Every commit (CI gate)

**autonomous-dev components**:
- `tests/integration/` — component interaction tests
- `tests/regression/regression/` — released feature regression tests
- Hook wiring tests (hook registered in settings AND exists on disk AND documented)

**Role in diamond**: Generated from acceptance criteria. When acceptance tests define "user can batch-implement 3 issues", integration tests verify the individual steps: worktree creation, issue fetching, sequential processing, merge.

### Layer 5: LLM-as-Judge Evaluation

**What**: Semantic validation — does code match intent?
**Speed**: < 2 minutes
**Determinism**: Probabilistic (~80-90% human agreement)
**Runs**: Pre-release, nightly, or on-demand (`pytest --genai`)

**autonomous-dev components**:
- `tests/genai/` — 16+ test files with LLM-as-judge
- `GenAIClient` in `conftest.py` — OpenRouter-backed, dual model (Gemini Flash + Haiku 4.5)
- 24h response caching, ~$0.02/run

**Evaluation types**:
- **Congruence**: Do file pairs agree? (`implement.md` ↔ `implementer.md`)
- **Architecture**: Do components match docs?
- **Security posture**: No secrets, proper exit codes
- **Doc completeness**: All components documented?

**Reliability data** (ICLR 2025, arXiv 2510.09738):
- GPT-4 level models: 85% agreement with human judgment
- Cohen's Kappa baseline: human-to-human κ = 0.801
- 27 of 54 tested LLMs achieve Tier 1 (human-like judgment, κ = 0.753-0.806)
- Production threshold: κ ≥ 0.80 OR human-like performance (|z| < 1)

**Best practices**:
- Few-shot examples increase consistency 65% → 77.5%
- Majority voting (3 runs, take consensus) for critical decisions
- Rubric-based scoring (0-10), not binary pass/fail
- Cache responses to eliminate redundant API costs

### Layer 6: Acceptance Criteria (Top)

**What**: Business intent, user-defined "done"
**Speed**: Varies (human review + LLM evaluation)
**Determinism**: Human-defined criteria, LLM-evaluated
**Runs**: Per feature (before implementation in acceptance-first mode)

**autonomous-dev components**:
- PROJECT.md GOALS/SCOPE/CONSTRAINTS — strategic acceptance criteria
- Issue acceptance criteria — feature-level acceptance criteria
- `/scaffold-genai-uat` — generates acceptance test infrastructure

**Role in diamond**: The primary driver. Acceptance criteria define what "done" means from the user's perspective. All lower layers exist to satisfy and lock in the acceptance criteria. Unit tests are derived artifacts; acceptance criteria are specifications.

---

## How Layers Map to autonomous-dev

| Layer | Component | Determinism | CI Gate? |
|-------|-----------|-------------|----------|
| Type/Lint | `auto_format.py`, `security_scan.py` | 100% | Yes |
| Unit Tests | `tests/unit/`, `tests/regression/smoke/` | 100% | Yes |
| Properties | Hook invariants, manifest sync, structure checks | 100% | Yes |
| Integration | `tests/integration/`, hook wiring tests | 100% | Yes |
| LLM-as-Judge | `tests/genai/` (16+ files) | ~85% | Optional |
| Acceptance | PROJECT.md, issue criteria, GenAI UAT | Human-defined | Per feature |

---

## The Paradigm Shift

### Traditional TDD (Pyramid)
```
Write unit tests → Implement → Validate
```
- Tests define implementation details
- Agents game unit tests (satisfy letter, not spirit)
- Brittle — hardcoded counts break on any change

### Diamond Model (Acceptance-First)
```
Define acceptance criteria → Implement + generate unit tests → Validate all layers
```
- Acceptance criteria define business outcomes
- Unit tests lock in behavior (regression prevention)
- Each layer serves a different purpose

### What Changed
| Aspect | Pyramid | Diamond |
|--------|---------|---------|
| Primary driver | Unit tests | Acceptance criteria |
| Unit test role | Specification | Regression lock |
| Generation order | Tests first → code | Criteria first → code + tests |
| Agent gaming risk | High (game unit tests) | Low (can't game acceptance criteria) |
| Determinism | 100% required | Mixed (deterministic floor + probabilistic middle) |

---

## Data Supporting the Diamond

| Finding | Source | Implication |
|---------|--------|-------------|
| Claude Opus 4.5: 80.9% SWE-bench (65% improvement in 13 months) | Anthropic/HuggingFace | Models good enough to generate unit tests post-implementation |
| AI PRs have 1.7x more issues than human PRs | CodeRabbit 2025 | Need stronger specification layer (acceptance criteria) |
| 95% of generated acceptance test scenarios rated helpful | AutoUAT Study (arXiv, 2025) | LLM-generated acceptance tests are production-viable |
| LLM-as-judge: 80-90% human agreement | ICLR 2025 (arXiv 2510.09738) | Sufficient for semantic validation layer |
| Property-based testing: +23-37% pass@1 improvement | arXiv 2506.18315 | Properties catch bugs unit tests miss |
| AI productivity paradox: 21% more tasks, 91% longer reviews | Faros AI (10K+ developers) | Current SDLC can't absorb AI speed — need redesigned quality gates |
| Only 3% of developers "highly trust" AI outputs | iqode 2025 | Trust comes from verification layers, not hope |

---

## Migration Path

### Current State (v3.51.0)
- Two-layer testing: Traditional (deterministic) + GenAI (semantic)
- TDD-first pipeline (test-master writes tests before implementation)
- Binary pass/fail on all tests

### Target State
- Six-layer diamond: Deterministic floor + probabilistic middle + acceptance top
- Acceptance-first option (`/implement --acceptance-first`) — Issue #350 ✅
- Soft-failure thresholds (`SoftFailureTracker`, `thresholds.json`, `--strict-genai`) — Issue #351 ✅
- Property-based invariants explicitly codified

### Steps
1. **Document diamond model** (this file) — Issue #352 ✅
2. **Add soft-failure thresholds** — Issue #351 ✅
3. **Add acceptance-first pipeline mode** — Issue #350 ✅
4. **Codify property-based invariants** — future issue
5. **Retire brittle hardcoded tests** — ongoing

---

## Decision Framework: When to Write Which Test

| Question | Test Type |
|----------|-----------|
| "Does this function return the right value?" | Unit test (Layer 2) |
| "Does output satisfy an invariant across all inputs?" | Property test (Layer 3) |
| "Do these components work together?" | Integration test (Layer 4) |
| "Does the code match the documented architecture?" | GenAI congruence test (Layer 5) |
| "Does this feature satisfy the user's requirement?" | Acceptance test (Layer 6) |
| "Is this code formatted correctly?" | Lint/type check (Layer 1) |

---

**For implementation details**: See [PROJECT.md](../PROJECT.md) Architecture section
**For GenAI test infrastructure**: See `/scaffold-genai-uat` command
**For test patterns**: See `plugins/autonomous-dev/skills/testing-guide/SKILL.md`
