"""
Integration tests for enhanced test coverage workflow (Issue #161)

Tests validate the complete workflow from acceptance criteria parsing
through test generation, organization, and validation.

These tests follow TDD - they should FAIL until implementation is complete.

Run with: pytest tests/integration/test_enhanced_test_coverage_workflow.py --tb=line -q
"""

import json
import subprocess
from pathlib import Path
from typing import Dict, List
from unittest.mock import Mock, patch, MagicMock

import pytest


class TestEnhancedTestCoverageWorkflow:
    """Integration tests for complete enhanced test coverage workflow."""

    @pytest.fixture
    def mock_github_issue(self) -> Dict[str, str]:
        """Mock GitHub issue with categorized acceptance criteria."""
        return {
            "number": 161,
            "title": "Enhanced test-master agent",
            "body": """
## Summary
Enhanced test-master agent for comprehensive test coverage.

## Acceptance Criteria

### Fresh Install
- [ ] Test-master generates UAT tests from acceptance criteria (1 per criterion)
- [ ] UAT tests use pytest-bdd Gherkin-style format
- [ ] Tests written to tests/uat/ directory

### Update
- [ ] Integration tests cover module boundaries
- [ ] Integration tests written to tests/integration/ directory

### Validation
- [ ] Unit tests have GREEN implementations (not skeletons)
- [ ] Unit tests written to tests/unit/ directory
- [ ] Test validation gate runs before reviewer agent

### Security
- [ ] All tests pass security validation
- [ ] No hardcoded credentials in tests
- [ ] Commit blocked if any test fails
"""
        }

    @pytest.fixture
    def mock_batch_state(self, tmp_path) -> Dict:
        """Mock batch state with test metrics."""
        return {
            "batch_id": "batch-20251225-123456",
            "features_file": str(tmp_path / "features.txt"),
            "total_features": 1,
            "features": ["Enhanced test-master agent"],
            "current_index": 0,
            "test_metrics": {
                "unit_tests_generated": 0,
                "integration_tests_generated": 0,
                "uat_tests_generated": 0,
                "total_tests": 0,
                "tests_passed": 0,
                "tests_failed": 0
            }
        }

    # =============================================================================
    # End-to-End Workflow Tests
    # =============================================================================

    def test_acceptance_criteria_to_uat_mapping(self, tmp_path, mock_github_issue):
        """
        GIVEN: GitHub issue with categorized acceptance criteria
        WHEN: Executing full pipeline (fetch → parse → format → generate)
        THEN: UAT tests are generated for each acceptance criterion
        """
        # Import modules (will fail until implementation exists)
        from autonomous_dev.lib.acceptance_criteria_parser import (
            fetch_issue_body,
            parse_acceptance_criteria,
            format_for_uat
        )

        # Arrange
        issue_number = 161

        # Mock gh CLI
        with patch('subprocess.run') as mock_run:
            mock_run.return_value = Mock(
                returncode=0,
                stdout=mock_github_issue["body"],
                stderr=""
            )

            # Act - Step 1: Fetch issue body
            issue_body = fetch_issue_body(issue_number)

            # Act - Step 2: Parse acceptance criteria
            criteria = parse_acceptance_criteria(issue_body)

            # Act - Step 3: Format for UAT
            uat_scenarios = format_for_uat(criteria)

            # Assert - Validate end-to-end pipeline
            assert len(criteria) == 4  # Fresh Install, Update, Validation, Security
            assert "Fresh Install" in criteria
            assert "Update" in criteria
            assert "Validation" in criteria
            assert "Security" in criteria

            # Validate UAT scenarios were generated
            assert len(uat_scenarios) > 0
            assert all("scenario_name" in s for s in uat_scenarios)
            assert all("category" in s for s in uat_scenarios)
            assert all("criterion" in s for s in uat_scenarios)

            # Count scenarios per category
            fresh_install_scenarios = [s for s in uat_scenarios if s["category"] == "Fresh Install"]
            assert len(fresh_install_scenarios) == 3  # 3 criteria in Fresh Install

    def test_tiered_directory_structure_created(self, tmp_path):
        """
        GIVEN: Test files generated by test-master
        WHEN: Organizing tests into tiers
        THEN: Creates tests/{unit,integration,uat}/ structure
        """
        from autonomous_dev.lib.test_tier_organizer import (
            create_tier_directories,
            organize_tests_by_tier,
            get_tier_statistics
        )

        # Act - Step 1: Create tier directories
        create_tier_directories(tmp_path)

        # Assert - Verify directory structure
        assert (tmp_path / "tests").exists()
        assert (tmp_path / "tests" / "unit").exists()
        assert (tmp_path / "tests" / "integration").exists()
        assert (tmp_path / "tests" / "uat").exists()

        # Act - Step 2: Create test files
        unit_test = tmp_path / "test_acceptance_criteria_parser.py"
        unit_test.write_text("def test_parse(): pass")

        integration_test = tmp_path / "test_full_pipeline.py"
        integration_test.write_text("def test_pipeline(): pass")

        uat_test = tmp_path / "test_uat_fresh_install.py"
        uat_test.write_text("from pytest_bdd import scenario\n")

        # Act - Step 3: Organize tests
        test_files = [unit_test, integration_test, uat_test]
        result = organize_tests_by_tier(test_files)

        # Assert - Verify organization
        assert "unit" in result
        assert "integration" in result
        assert "uat" in result

        # Act - Step 4: Get statistics
        stats = get_tier_statistics(tmp_path / "tests")

        # Assert - Verify statistics
        assert stats["unit"] == 1
        assert stats["integration"] == 1
        assert stats["uat"] == 1
        assert stats["total"] == 3

    def test_validation_gate_runs_tests(self, tmp_path):
        """
        GIVEN: Complete test suite (unit + integration + UAT)
        WHEN: Running validation gate
        THEN: Executes all tests and reports results
        """
        from autonomous_dev.lib.test_tier_organizer import create_tier_directories
        from autonomous_dev.lib.test_validator import run_validation_gate

        # Arrange - Create test structure
        create_tier_directories(tmp_path)

        # Create passing unit tests
        unit_test = tmp_path / "tests" / "unit" / "test_example.py"
        unit_test.write_text("def test_unit(): assert True\n")

        # Create passing integration tests
        integration_test = tmp_path / "tests" / "integration" / "test_workflow.py"
        integration_test.write_text("def test_integration(): assert True\n")

        # Create passing UAT tests
        uat_test = tmp_path / "tests" / "uat" / "test_acceptance.py"
        uat_test.write_text("def test_uat(): assert True\n")

        # Mock subprocess.run (pytest execution)
        with patch('subprocess.run') as mock_run:
            mock_run.return_value = Mock(
                returncode=0,
                stdout="3 passed in 0.05s",
                stderr=""
            )

            # Act - Run validation gate
            result = run_validation_gate(tmp_path / "tests")

            # Assert - Validate gate passed
            assert result["gate_passed"] is True
            assert result["all_tests_passed"] is True
            assert result["passed"] == 3

    def test_batch_state_tracks_test_metrics(self, tmp_path, mock_batch_state):
        """
        GIVEN: Batch processing with test generation
        WHEN: Recording test metrics
        THEN: Batch state tracks test counts per tier
        """
        from autonomous_dev.lib.batch_state_manager import (
            save_batch_state,
            load_batch_state,
            update_test_metrics
        )

        # Arrange - Save initial batch state
        batch_state_file = tmp_path / "batch_state.json"
        batch_state_file.write_text(json.dumps(mock_batch_state))

        # Act - Update test metrics
        update_test_metrics(
            batch_state_file,
            unit_tests=10,
            integration_tests=5,
            uat_tests=8
        )

        # Act - Load updated state
        updated_state = load_batch_state(batch_state_file)

        # Assert - Verify metrics tracked
        assert "test_metrics" in updated_state
        metrics = updated_state["test_metrics"]
        assert metrics["unit_tests_generated"] == 10
        assert metrics["integration_tests_generated"] == 5
        assert metrics["uat_tests_generated"] == 8
        assert metrics["total_tests"] == 23

    # =============================================================================
    # Test Generation Integration
    # =============================================================================

    def test_test_master_generates_unit_tests(self, tmp_path, mock_github_issue):
        """
        GIVEN: Implementation plan from planner agent
        WHEN: test-master generates unit tests
        THEN: Creates GREEN unit tests (not skeletons)
        """
        from autonomous_dev.lib.test_tier_organizer import create_tier_directories
        from autonomous_dev.lib.test_validator import run_tests

        # Arrange - Create test structure
        create_tier_directories(tmp_path)

        # Simulate test-master generating unit tests
        unit_test_file = tmp_path / "tests" / "unit" / "lib" / "test_acceptance_criteria_parser.py"
        unit_test_file.parent.mkdir(parents=True, exist_ok=True)
        unit_test_file.write_text("""
def test_parse_acceptance_criteria():
    from autonomous_dev.lib.acceptance_criteria_parser import parse_acceptance_criteria

    # Arrange
    issue_body = "## Acceptance Criteria\\n- [ ] Criterion 1"

    # Act
    result = parse_acceptance_criteria(issue_body)

    # Assert
    assert isinstance(result, dict)
""")

        # Mock subprocess.run
        with patch('subprocess.run') as mock_run:
            mock_run.return_value = Mock(
                returncode=0,  # Tests should fail in RED phase
                stdout="1 passed in 0.01s",
                stderr=""
            )

            # Act - Run tests
            result = run_tests(tmp_path / "tests" / "unit")

            # Assert - Tests exist and are executable (RED phase - will fail)
            assert unit_test_file.exists()
            assert "def test_" in unit_test_file.read_text()

    def test_test_master_generates_integration_tests(self, tmp_path):
        """
        GIVEN: Module boundaries identified by researcher
        WHEN: test-master generates integration tests
        THEN: Creates integration tests for module interactions
        """
        from autonomous_dev.lib.test_tier_organizer import create_tier_directories

        # Arrange - Create test structure
        create_tier_directories(tmp_path)

        # Simulate test-master generating integration test
        integration_test = tmp_path / "tests" / "integration" / "test_acceptance_to_uat_pipeline.py"
        integration_test.parent.mkdir(parents=True, exist_ok=True)
        integration_test.write_text("""
def test_full_pipeline():
    from autonomous_dev.lib.acceptance_criteria_parser import fetch_issue_body, parse_acceptance_criteria
    from autonomous_dev.lib.test_tier_organizer import format_for_uat

    # Integration test across modules
    body = fetch_issue_body(161)
    criteria = parse_acceptance_criteria(body)
    uat = format_for_uat(criteria)

    assert len(uat) > 0
""")

        # Assert - Integration test created
        assert integration_test.exists()
        assert "def test_full_pipeline" in integration_test.read_text()

    def test_test_master_generates_uat_tests_from_criteria(self, tmp_path, mock_github_issue):
        """
        GIVEN: Acceptance criteria from GitHub issue
        WHEN: test-master generates UAT tests
        THEN: Creates 1 UAT test per acceptance criterion
        """
        from autonomous_dev.lib.acceptance_criteria_parser import (
            parse_acceptance_criteria,
            format_for_uat
        )
        from autonomous_dev.lib.test_tier_organizer import create_tier_directories

        # Arrange - Parse acceptance criteria
        criteria = parse_acceptance_criteria(mock_github_issue["body"])
        uat_scenarios = format_for_uat(criteria)

        # Create test structure
        create_tier_directories(tmp_path)

        # Act - Generate UAT test files
        uat_dir = tmp_path / "tests" / "uat"
        for scenario in uat_scenarios:
            test_file = uat_dir / f"{scenario['scenario_name']}.py"
            test_file.write_text(f"""
from pytest_bdd import scenario, given, when, then

@scenario('features/test_coverage.feature', '{scenario['criterion']}')
def test_{scenario['scenario_name']}():
    pass

@given('A GitHub issue with acceptance criteria')
def github_issue():
    return {mock_github_issue['number']}

@when('test-master processes the issue')
def process_issue(github_issue):
    # Process
    pass

@then('{scenario['criterion']}')
def verify_criterion():
    assert True
""")

        # Assert - One UAT test per criterion
        uat_files = list(uat_dir.glob("test_*.py"))
        assert len(uat_files) == len(uat_scenarios)

    # =============================================================================
    # Validation Gate Integration
    # =============================================================================

    def test_validation_gate_blocks_on_failures(self, tmp_path):
        """
        GIVEN: Test suite with failures
        WHEN: Running validation gate
        THEN: Blocks commit and prevents reviewer agent execution
        """
        from autonomous_dev.lib.test_tier_organizer import create_tier_directories
        from autonomous_dev.lib.test_validator import run_validation_gate

        # Arrange - Create failing test
        create_tier_directories(tmp_path)
        failing_test = tmp_path / "tests" / "unit" / "test_failing.py"
        failing_test.write_text("def test_fail(): assert False\n")

        # Mock subprocess.run
        with patch('subprocess.run') as mock_run:
            mock_run.return_value = Mock(
                returncode=1,
                stdout="1 failed in 0.01s",
                stderr=""
            )

            # Act - Run validation gate
            result = run_validation_gate(tmp_path / "tests")

            # Assert - Gate blocks
            assert result["gate_passed"] is False
            assert result["block_commit"] is True
            assert result["failed"] == 1

    def test_validation_gate_runs_before_reviewer_agent(self, tmp_path):
        """
        GIVEN: /auto-implement workflow at CHECKPOINT 4.1
        WHEN: Validation gate executes
        THEN: Runs before reviewer, security-auditor, doc-master
        """
        from autonomous_dev.lib.test_tier_organizer import create_tier_directories
        from autonomous_dev.lib.test_validator import run_validation_gate

        # Arrange - Create passing tests
        create_tier_directories(tmp_path)
        test_file = tmp_path / "tests" / "unit" / "test_example.py"
        test_file.write_text("def test_pass(): assert True\n")

        # Mock subprocess.run
        with patch('subprocess.run') as mock_run:
            mock_run.return_value = Mock(
                returncode=0,
                stdout="1 passed in 0.01s",
                stderr=""
            )

            # Act - Run validation gate (simulates CHECKPOINT 4.1)
            result = run_validation_gate(tmp_path / "tests")

            # Assert - Gate passes, workflow continues
            assert result["gate_passed"] is True
            assert result["all_tests_passed"] is True

    # =============================================================================
    # TDD Red Phase Validation
    # =============================================================================

    def test_red_phase_validation_blocks_premature_green(self, tmp_path):
        """
        GIVEN: Tests that pass before implementation exists
        WHEN: Running red phase validation
        THEN: Blocks workflow and reports TDD violation
        """
        from autonomous_dev.lib.test_tier_organizer import create_tier_directories
        from autonomous_dev.lib.test_validator import run_tests, validate_red_phase

        # Arrange - Create passing test (BAD in red phase)
        create_tier_directories(tmp_path)
        premature_test = tmp_path / "tests" / "unit" / "test_premature.py"
        premature_test.write_text("def test_premature(): assert True\n")

        # Mock subprocess.run
        with patch('subprocess.run') as mock_run:
            mock_run.return_value = Mock(
                returncode=0,
                stdout="1 passed in 0.01s",
                stderr=""
            )

            # Act - Run tests
            result = run_tests(tmp_path / "tests")

            # Assert - Red phase validation should block
            with pytest.raises(ValueError, match="TDD red phase violation"):
                validate_red_phase(result)

    def test_red_phase_validation_accepts_import_errors(self, tmp_path):
        """
        GIVEN: Tests with import errors (module doesn't exist)
        WHEN: Running red phase validation
        THEN: Accepts as valid red phase
        """
        from autonomous_dev.lib.test_tier_organizer import create_tier_directories
        from autonomous_dev.lib.test_validator import run_tests, validate_red_phase

        # Arrange - Create test with import error
        create_tier_directories(tmp_path)
        import_error_test = tmp_path / "tests" / "unit" / "test_import_error.py"
        import_error_test.write_text("""
def test_import_error():
    from nonexistent_module import function
    assert function() == True
""")

        # Mock subprocess.run
        with patch('subprocess.run') as mock_run:
            mock_run.return_value = Mock(
                returncode=2,
                stdout="collected 0 items / 1 error",
                stderr="ImportError: cannot import name 'nonexistent_module'"
            )

            # Act - Run tests
            result = run_tests(tmp_path / "tests")

            # Assert - Should accept import errors in red phase
            validate_red_phase(result)  # Should not raise

    # =============================================================================
    # Test Metrics Tracking
    # =============================================================================

    def test_batch_state_records_test_generation_counts(self, tmp_path, mock_batch_state):
        """
        GIVEN: test-master generates tests for a feature
        WHEN: Recording test generation metrics
        THEN: Batch state tracks unit/integration/UAT counts
        """
        from autonomous_dev.lib.batch_state_manager import (
            save_batch_state,
            load_batch_state,
            update_test_metrics
        )

        # Arrange - Save initial state
        batch_state_file = tmp_path / "batch_state.json"
        batch_state_file.write_text(json.dumps(mock_batch_state))

        # Act - Record test generation
        update_test_metrics(
            batch_state_file,
            unit_tests=15,
            integration_tests=8,
            uat_tests=10
        )

        # Act - Load state
        state = load_batch_state(batch_state_file)

        # Assert - Metrics recorded
        assert state["test_metrics"]["unit_tests_generated"] == 15
        assert state["test_metrics"]["integration_tests_generated"] == 8
        assert state["test_metrics"]["uat_tests_generated"] == 10
        assert state["test_metrics"]["total_tests"] == 33

    def test_batch_state_records_test_execution_results(self, tmp_path, mock_batch_state):
        """
        GIVEN: Validation gate executes tests
        WHEN: Recording test results
        THEN: Batch state tracks pass/fail counts
        """
        from autonomous_dev.lib.batch_state_manager import (
            save_batch_state,
            load_batch_state,
            update_test_results
        )

        # Arrange - Save initial state
        batch_state_file = tmp_path / "batch_state.json"
        batch_state_file.write_text(json.dumps(mock_batch_state))

        # Act - Record test results
        update_test_results(
            batch_state_file,
            passed=28,
            failed=3,
            errors=2
        )

        # Act - Load state
        state = load_batch_state(batch_state_file)

        # Assert - Results recorded
        assert state["test_metrics"]["tests_passed"] == 28
        assert state["test_metrics"]["tests_failed"] == 3
        assert state["test_metrics"]["tests_errors"] == 2

    # =============================================================================
    # Error Handling
    # =============================================================================

    def test_graceful_degradation_no_acceptance_criteria(self, tmp_path):
        """
        GIVEN: GitHub issue without acceptance criteria section
        WHEN: Parsing acceptance criteria
        THEN: Gracefully degrades and generates basic tests only
        """
        from autonomous_dev.lib.acceptance_criteria_parser import (
            parse_acceptance_criteria,
            format_for_uat
        )

        # Arrange - Issue without acceptance criteria
        issue_body = """
## Summary
Simple feature without acceptance criteria.

## Implementation Details
Just implement it.
"""

        # Act
        criteria = parse_acceptance_criteria(issue_body)
        uat_scenarios = format_for_uat(criteria)

        # Assert - Graceful degradation
        assert isinstance(criteria, dict)
        assert len(criteria) == 0
        assert len(uat_scenarios) == 0  # No UAT tests generated

    def test_handles_validation_gate_timeout(self, tmp_path):
        """
        GIVEN: Long-running test suite
        WHEN: Validation gate times out
        THEN: Blocks commit and reports timeout
        """
        from autonomous_dev.lib.test_tier_organizer import create_tier_directories
        from autonomous_dev.lib.test_validator import run_validation_gate

        # Arrange
        create_tier_directories(tmp_path)
        test_file = tmp_path / "tests" / "unit" / "test_example.py"
        test_file.write_text("def test_ex(): assert True\n")

        # Mock subprocess.run to timeout
        with patch('subprocess.run') as mock_run:
            mock_run.side_effect = subprocess.TimeoutExpired(
                cmd="pytest",
                timeout=300
            )

            # Act & Assert
            with pytest.raises(TimeoutError, match="5 minutes"):
                run_validation_gate(tmp_path / "tests", timeout=300)
